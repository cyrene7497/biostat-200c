---
title: "Biostat 200C Homework 1"
author: "Cyrene Arputhasamy"
subtitle: Due Apr 14 @ 11:59PM
output: 
  html_document:
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

To submit homework, please submit Rmd and html files to BruinLearn by the deadline.


## Q1. Analysis of Swiss data using Fertility as the response variable
```{r}
sessionInfo()
```
```{r}
library(tidyverse)
library(gtsummary)
library(car)
```
```{r}
# Rename rows since each row is a province in Switzerland. 
swiss1 <- swiss %>% 
  as_tibble(rownames = "Province") %>% 
  print(width = Inf)
str(swiss1)
```
### Initial Data Analysis

#### Numerical Characteristics
```{r}
swiss1 %>% subset(select = -c(Province)) %>% 
  tbl_summary(type = all_continuous() ~ "continuous2",
              statistic = all_continuous() ~ 
                c("{N_nonmiss}", "{median} ({p25}, {p75})", "{min}, {max}"),
              digits = all_continuous() ~ 2)
```
All variables are numeric besides `Province`. All except for `Fertility` are percentages based on proportions of the population. `Fertility` is the common standardized fertility measure $I_g$, general fertility which is equal to the total number of children born to married women divided by the maximum conceivable number of births based on the expected fertility of the Hutterites, an Anabaptist sect who practiced universal marriage and no birth control in the early twentieth century (representing maximum expected fertility). `Agriculture` is the percentage of males involved in agriculture as occupation. `Examination` is the percentage of draftees receiving highest mark on army examination. `Education` is the percentage of draftees who had an education beyond primary school. `Catholic` is the percent Catholic as opposed to Protestant. `Infant.Mortality` is the percent of live births who lived less than 1 year. 

#### Graphical Characteristics
```{r}
ggplot(data = swiss1) +
  geom_histogram(mapping = aes(x = Fertility)) + 
  labs(x = "Fertility Measure", y = "Number of Provinces") + 
  theme_bw()
```
At first glance it appears that there might be some outliers of unusually low and high fertility. 
```{r}
for (var in c("Agriculture", "Examination", "Education", "Catholic", "Infant.Mortality")) {
  plot <- ggplot(data = swiss1) + 
    geom_point(mapping = aes(x = get(var), y = Fertility)) +
    labs(x = var) + ggtitle(var) + theme_bw()
  print(plot)
}
```
There appears to be a distinct negative trend between `Examination` (% draftees receiving highest mark on army examination) and `Fertility`. Many of the provinces have a low percentage (0-20%) of draftees who were educated beyond primary school. There seems like there might be a relationship between `Fertility` and `Education`. The percent Catholic as opposed to Protestant seems to be in two groups. Generally provinces either had very high percentage of Catholics or quite low. There are few provinces with nearly 50-50 proportion between the two religions. We can try splitting `Catholic` into three levels `low`, `middle`, and `high`. 
```{r}
swiss1 <- swiss1 %>% 
  mutate(CatholicLevels = case_when(Catholic < 25 ~ "low",
                                    25 < Catholic & Catholic < 75 ~ "middle",
                                    75 < Catholic ~ "high")) %>%
  mutate(CatholicLevels = as.factor(CatholicLevels)) %>%
  print(width = Inf)
```
The `middle` level only has 3 observations so we can split it into 2 levels instead by the median. 
```{r}
swiss1 <- swiss1 %>% 
  mutate(Catholic2 = case_when(Catholic < 75 ~ "low",
                                    75 < Catholic ~ "high")) %>%
  mutate(Catholic2 = as.factor(Catholic2)) %>%
  print(width = Inf)
```
### Variable Selection
For variable selection we start with a big model (`biglm`) that includes all two-way interactions between predictors. By step-wise regression, we minimize the AIC (Akaike Information Criterion) to get a small model (`smalllm`). 
```{r}
lmod <- lm(Fertility ~ Agriculture + Examination + Education + Catholic2 + Infant.Mortality, swiss1)
summary(lmod)
biglm <- lm(Fertility ~ (Agriculture + Examination + Education + Catholic2 + Infant.Mortality)^2, swiss1)
smalllm <- stats::step(biglm, trace = TRUE)
drop1(smalllm, test = "F")
```
The F-test shows that the `Agriculture:Examination` and `Agriculture:Education` interaction terms can be dropped from the model. Thus the model after dropping these interactions is:
```{r}
lmodf <- lm(Fertility ~ Agriculture + Examination + Education + Catholic2 + 
              Infant.Mortality + Agriculture:Infant.Mortality + Examination:Education + 
              Examination:Infant.Mortality, swiss1)
summary(lmodf)
```

### Diagnostics 
```{r}
plot(lmodf)
```
It appears the model assumptions are generally met. The Residuals vs Fitted plot shows the residuals randomly around zero. This means we are roughly meeting the linearity and constant variance assumptions. The Normal Q-Q plot is almost linear so we've met the normality assumption as well.

We can also look at the partial residuals for each component in the model without the interactions:
```{r}
crPlots(lmod)
```
The partial residual plots look mostly linear, but we can explore transforming `Examination`.

### Exploration of Transformations

The previous scatter plot of `Examination` and `Fertility` indicated that it might be helpful to transform `Examination` down the ladder of powers. 
```{r}
ggplot(data = swiss1) + 
    geom_point(mapping = aes(x = Examination^(-.5), y = Fertility))
```
The transformation seems to have made the scatterplot a little more linear.
```{r}
swiss1 <- swiss1 %>%
  mutate(TExam = Examination^(-.5)) %>%
  print(width = Inf)
trlm <- lm(Fertility ~ Agriculture + TExam + Education + 
             Catholic2 + Infant.Mortality, swiss1)
crPlots(trlm)
```
The fit doesn't improve the diagnostics, so we can continue without transformations. 

### Final Model
```{r}
lmodf %>%
  tbl_regression() %>%
  bold_labels() %>%
  bold_p(t = 0.05)
```
#### Diagnostics
```{r}
plot(lmodf)
swiss1 %>%
  mutate(cook = cooks.distance(lmodf)) %>%
  filter(cook >= 0.1) %>%
  print(width = Inf)
```
There are some observations with cook's distances higher than 0.1 but there's no clear reason why that might be the case, and no indication that they might be outliers. 

### Predictions 

- Some predictions of future observations for interesting values of the predictors.
Let's predict the fertility of a few random provinces in Switzerland with median `Agriculture`; `Examination` at the observed minimum, 1st quartile, median, 3rd quartile, and maximum; minimum `Education`; either level of `Catholic` and maximum `Infant.Mortality`.
```{r}
pdf <- tibble(Agriculture = rep(median(swiss1$Agriculture), 5),
              Examination = c(3, 12, 16, 22, 37),
              Education = rep(1, 5),
              Catholic2 = c("high", "low", "high", "low", "high"),
              Infant.Mortality = rep(26.6, 5))
print(pdf)
pp <- predict(lmodf, new = pdf)
pdf %>% mutate(PredictedFertility = pp)
```
It's interesting that a hypothetical province with average percentage of males involved in agriculture, maximum percentage of draftees with highest marks on army examination, lowest percentage of education beyond primary school, high percentage of Catholics, and maximum observed infant mortality has a predicted fertility that would be even higher than the Hutterites since the percent fertility is expected to be above 100%. 

### Conclusion

  + **BACKGROUND:** The `swiss` data comes from a study of `Swiss Fertility and Socioeconomic Indicators (1888)`. This study used a standardized fertility measure and socioeconomic indicators for each of 47 French-speaking provinces of Switzerland at about 1888 at a time when Switzerland was entering a period known as a demographic transition. Its fertility was beginning to fall from the high level typical of underdeveloped countries. 
  
  + **OBJECTIVE:** This study aimed to identify the factors influencing fertility and the observed decline in fertility in Switzerland, taking into account the variety of socioeconomic factors across the provinces.
  
  + **METHODS:** Data was collected from the Office of Population Research. A linear regression analysis was performed to explore the factors contributing to fertility. All variables but `Fertility` give proportions of the population. The original `Catholic` variable was turned into a categorical variable of two levels (`Catholic2`) since there were two distinct populations among the provinces -- those with more than 75% of draftees being Catholic, and those with less than 75% Catholic. 
  
  + **RESULTS:** The study found that, for provinces in Switzerland in 1888, the percentage of males involved in agriculture as an occupation, the percent of draftees receiving highest mark on army examination, percent education beyond primary school for draftees, whether there was a high or low percentage of Catholics as opposed to Protestants, and percent of live births who lived less than 1 year were significant predictors of fertility. 
  
  + **CONCLUSIONS:** Overall the study found that agriculture, army examination, education, Catholicism, and infant mortality had negative $\beta$ values, indicating that their relationship with `Fertility` was negative. For example, a decrease in percentage of males involved in agriculture as an occupation would be associated with an increase in fertility on average, when holding all other factors constant. This might be due to provinces in Switzerland becoming more developed at the time of the data collection. 


## Q2. Concavity of logistic regression log-likelihood 

### Q2.1

Write down the log-likelihood function of logistic regression for Bernoulli responses.

Given $n$ data points $(y_i, \mathbf{x}_i)$, $i=1,\ldots,n$, the **log-likelihood** is
\begin{eqnarray*}
\ell(\boldsymbol{\beta}) &=& \sum_i \log \left[p_i^{y_i} (1 - p_i)^{1 - y_i}\right] \\
&=& \sum_i \left[ y_i \log p_i + (1 - y_i) \log (1 - p_i) \right] \\
&=& \sum_i \left[ y_i \log \frac{e^{\eta_i}}{1 + e^{\eta_i}} + (1 - y_i) \log \frac{1}{1 + e^{\eta_i}}  \right] \\
&=& \sum_i \left[ y_i \eta_i - \log (1 + e^{\eta_i}) \right] \\
&=& \sum_i \left[ y_i \cdot \mathbf{x}_i^T \boldsymbol{\beta} - \log (1 + e^{\mathbf{x}_i^T \boldsymbol{\beta}}) \right].
\end{eqnarray*}

### Q2.2

Derive the gradient vector and Hessian matrix of the log-likelihood function with respect to the regression coefficients $\boldsymbol{\beta}$. 

\begin{eqnarray*}
\ell(\boldsymbol{\beta}, x_i, y_i) &=& \sum_i \left[ y_i \cdot \mathbf{x}_i^T \boldsymbol{\beta} - \log (1 + e^{\mathbf{x}_i^T \boldsymbol{\beta}}) \right] \\
\nabla_\beta(\mathbf{x}_i^T \boldsymbol{\beta}) &=& \nabla_\beta(\mathbf{x}_{i0} \beta_0 + \mathbf{x}_{i1} \beta_1 + \cdots + \mathbf{x}_{ip} \beta_p) 
&=& \boldsymbol{\mathbf{x}_i} \\
\nabla_\beta \ell(\cdot) &=& \sum_i y_i(\nabla_\beta \mathbf{x}_i^T \boldsymbol{\beta}) - \nabla_\beta (\log (1 + e^{\mathbf{x}_i^T \boldsymbol{\beta}})) \\
&=& \sum_i y_i \mathbf{x}_i - \frac{1}{1 + e^{\mathbf{x}_i^T \boldsymbol{\beta}}} \cdot e^{\mathbf{x}_i^T \boldsymbol{\beta}} \cdot \nabla_\beta \mathbf{x}_i^T \boldsymbol{\beta} \\
&=& \sum_i y_i \mathbf{x}_i - \frac{e^{\mathbf{x}_i^T \boldsymbol{\beta}}}{1 + e^{\mathbf{x}_i^T \boldsymbol{\beta}}} \cdot \mathbf{x}_i \\
&=& \sum_i y_i \mathbf{x}_i - \frac{1}{1 + e^{- \mathbf{x}_i^T \boldsymbol{\beta}}} \cdot \mathbf{x}_i
\end{eqnarray*}

The Hessian is the matrix of the partial derivatives of the gradient vector with respect to $\boldsymbol{\beta}$.

\begin{eqnarray*}
\nabla_\beta(\frac{\partial \ell(\cdot)}{\partial \beta_j}) &=& \sum_i \nabla_\beta(y_i x_{ij} - \frac{1}{1 + e^{- \mathbf{x}_i^T \boldsymbol{\beta}}} x_{ij}) \\
&=& \sum_i \nabla_\beta(-x_{ij}(1 + e^{- \mathbf{x}_i^T \boldsymbol{\beta}})^{-1}) \\
&=& \sum_i -x_{ij} \cdot -1(1 + e^{- \mathbf{x}_i^T \boldsymbol{\beta}})^{-2} e^{- \mathbf{x}_i^T \boldsymbol{\beta}} \mathbf{x}_i \\
&=& - \sum_i x_{ij} \frac{e^{- \mathbf{x}_i^T \boldsymbol{\beta}}}{1 + e^{- \mathbf{x}_i^T \boldsymbol{\beta}}} \mathbf{x}_i \\
&=& - \sum_i \begin{bmatrix}
x_{i0} \frac{e^{- \mathbf{x}_i^T \boldsymbol{\beta}}}{(1 + e^{- \mathbf{x}_i^T \boldsymbol{\beta}})^{2}} \mathbf{x}_i & \cdots & x_{ip} \frac{e^{- \mathbf{x}_i^T \boldsymbol{\beta}}}{(1 + e^{- \mathbf{x}_i^T \boldsymbol{\beta}})^{2}} \mathbf{x}_i \\
\end{bmatrix} \\
&=& - \sum_i \frac{e^{- \mathbf{x}_i^T \boldsymbol{\beta}}}{(1 + e^{- \mathbf{x}_i^T \boldsymbol{\beta}})^{2}} \begin{bmatrix}
x_{i0} \mathbf{x}_i & \cdots & x_{ip} \mathbf{x}_i
\end{bmatrix} \\
&=& - \sum_i \frac{e^{- \mathbf{x}_i^T \boldsymbol{\beta}}}{(1 + e^{- \mathbf{x}_i^T \boldsymbol{\beta}})^{2}} \mathbf{x}_i \begin{bmatrix}
x_{i0} & \cdots & x_{ip}
\end{bmatrix} \\
&=& - \sum_i \frac{e^{- \mathbf{x}_i^T \boldsymbol{\beta}}}{(1 + e^{- \mathbf{x}_i^T \boldsymbol{\beta}})^{2}} \mathbf{x}_i \mathbf{x}_i^T = H_\beta
\end{eqnarray*}

### Q2.3

Show that the log-likelihood function of logistic regression is a concave function in regression coefficients $\boldsymbol{\beta}$. (Hint: show that the negative Hessian is a positive semidefinite matrix.)

If we combine the scalar terms into a diagnoal matrix D and concatenate column vectors $x_i \in \mathbb{R}^{d}$ into a matrix $\mathbf{X}$ of size $d \times m$ such that $\sum_i^{m} \mathbf{x}_i \mathbf{x}_i^T = \mathbf{XX}^T$. The scalar terms are collected into a diagonal matrix $D$ such that $-H_\beta = XDX^T$. Let $\delta$ be any vector such that $\delta \in \mathbb{R}^{d}$. Then we can show that $\delta^T(-H_\beta)\delta \ge 0$ to show that the negative Hessian is a positive semi-definite matrix and thus the log-likelihood function of logistic regression is a concave function. 

\begin{eqnarray*}
\delta^T H \delta &=& \delta^TXDX^T\delta &=& \delta^TXD(\delta^TX)^T &=& \lVert \delta^T DX \rVert^{2} \ge 0
\end{eqnarray*}

## Q3.  

The National Institute of Diabetes and Digestive and Kidney Diseases conducted a study on 768 adult female Pima Indians living near Phoenix. The purpose of the study was to investigate factors related to diabetes. The data may be found in the the dataset `pima`.
```{r}
library(faraway)
library(tidyverse)
pima1 <- as_tibble(pima) %>%
  print(width = Inf)
str(pima1)
```

### Q3.1

Create a factor version of the test results and use this to produce an interleaved histogram to show how the distribution of insulin differs between those testing positive and negative. Do you notice anything unbelievable about the plot?
```{r}
pima1$test_f <- as.factor(pima1$test)
pima1$test_f2 <- pima1$test_f
levels(pima1$test_f2) <- c("negative", "positive")
ggplot(pima1, aes(x = insulin, fill = test_f2)) +
  geom_histogram(alpha =.5, position = "identity") +
  ylab("Count") + xlab("Insulin (mu U/ml)") +
  ggtitle("Histogram of Insulin by Diabetes Test Results") +
  theme_bw()
```
It's unusual that there are so many entries of 0 Insulin, even for both positive and negative tested patients. It's impossible to have 0 insulin in the body so the zeros must be missing values. 
### Q3.2

Replace the zero values of `insulin` with the missing value code `NA`. Re-create the interleaved histogram plot and comment on the distribution.
```{r}
pima1$insulin[pima1$insulin == 0] <- NA
ggplot(pima1, aes(x = insulin, fill = test_f2)) +
  geom_histogram(alpha =.5, position = "identity") +
  ylab("Count") + xlab("Insulin (mu U/ml)") +
  ggtitle("Histogram of Insulin by Diabetes Test Results") +
  theme_bw()
```
The peak insulin level of those who tested negative is now lower than that of those who tested positive for diabetes. The distribution of those with a negative test looks right skewed. There also looks like there might be some outliers with unusually high insulin levels. 

### Q3.3

Replace the incredible zeroes in other variables with the missing value code. Fit a model with the result of the diabetes test as the response and all the other variables as predictors. How many observations were used in the model fitting? Why is this less than the number of observations in the data frame?
```{r}
summary(pima1)
```
The summary shows that `glucose`, `diastolic`, `triceps`, and `bmi`also have zero values that don't make sense. 
```{r}
pima1$glucose[pima1$glucose == 0] <- NA
pima1$diastolic[pima1$diastolic == 0] <- NA
pima1$triceps[pima1$triceps == 0] <- NA
pima1$bmi[pima1$bmi == 0] <- NA
summary(pima1)
```
```{r}
# Diabetes Test Model
dtm <- glm(test_f ~ pregnant + glucose + diastolic + triceps + insulin + bmi + 
             diabetes + age,
           family = "binomial", data = pima1)
summary(dtm)
```
There were 392 observations used in the model fitting. This is different from the number of observations because 376 observations were omitted due to missingness. 

### Q3.4

Refit the model but now without the insulin and triceps predictors. How many observations were used in fitting this model? Devise a test to compare this model with that in the previous question.
```{r}
# Diabetes Test Model 2
dtm2 <- glm(test_f ~ pregnant + glucose + diastolic + bmi + diabetes + age,
           family = "binomial", data = pima1)
summary(dtm2)
```
The model without insulin or triceps used 724 observation. There were 44 observations omitted due to missingness. To compare this model with the previous model our null hypothesis is that the addition of `insulin` and `triceps` as predictors does not improve model fit, and the alternative hypothesis is that their addition does improve model fit. To assess whether the full model is superior to the reduced model (diabetes test model 2 versus model 1), we can take the differences in deviance of the two models and compute the chi-square test statistic. The residual deviance in model 1 is 344.02 on 383 degrees of freedom, and in model 2 is 672.86 on 717 degrees of freedom. So the chi-square test statistic is $672.86 - 344.02 = 328.84$ on $717 - 383 = 334$ degrees of freedom. We compute the p-value:
```{r}
dtm <- glm(test_f ~ pregnant + glucose + diastolic + triceps + insulin + bmi + 
             diabetes + age,
           family = "binomial", data = na.omit(pima1))
dtm2 <- glm(test_f ~ pregnant + glucose + diastolic + bmi + diabetes + age,
           family = "binomial", data = na.omit(pima1))
anova(dtm, dtm2, test = "Chi")
```
The p-value is high so we accept the null hypothesis that the addition of `insulin` and `triceps` as predictors does not improve model fit. 

### Q3.5

Use AIC to select a model. You will need to take account of the missing values. Which predictors are selected? How many cases are used in your selected model?
```{r}
bigdtm <- glm(test_f ~ pregnant + glucose + diastolic + triceps + insulin + bmi + 
             diabetes + age, family = binomial, data = na.omit(pima1))
smalldtm <- stats::step(bigdtm, trace = TRUE)
summary(smalldtm)
```
After selection by AIC, the predictors `pregnant`, `glucose`, `bmi`, `diabetes`, and `age` were selected to be in the model. There were 392 observations included in this model. 

### Q3.6

Create a variable that indicates whether the case contains a missing value. Use this variable as a predictor of the test result. Is missingness associated with the test result? Refit the selected model, but now using as much of the data as reasonable. Explain why it is appropriate to do this.

```{r}
pima1$has_na <- ifelse(apply(is.na(pima1), 1, sum) > 0, 1, 0)
missings.glm <- glm(test_f2 ~ has_na, family = binomial, data = pima1)
summary(missings.glm)
```
Since the missing values are not associated with the diabetes test outcome, we can assume that the missing values are occurring randomly. Therefore even with a smaller sample size after omitting NAs the effect size is still valid because missingness is not associated with the outcome. 

```{r}
pima1 <- pima1 %>%
  drop_na(pregnant) %>%
  drop_na(glucose) %>%
  drop_na(bmi) %>%
  drop_na(diabetes) %>%
  drop_na(age) %>%
  print(width = Inf)
dtm_f <- glm(test_f ~ pregnant + glucose + bmi + diabetes + age, 
             family = binomial, data = pima1)
summary(dtm_f)
```
As opposed to the previous fit of this model with 392 observations, this fit uses 752 observations.

### Q3.7

Using the last fitted model of the previous question, what is the difference in the odds of testing positive for diabetes for a woman with a BMI at the first quartile compared with a woman at the third quartile, assuming that all other factors are held constant? Give a confidence interval for this difference.

```{r}
confint(dtm_f)
```
We see a 95% confidence interval for the log-odds of the `bmi` coefficient is given by about $(0.05921, 0.11701)$. This corresponds to the change in log-odds for a 1-unit increase in `bmi` We want a 9.3 unit increase in BMI, so we multiply the endpoints of the interval by 9.3 to get a confidence interval of $(0.5506403, 1.088167)$. Exponentiating this result gives a confidence interval of about $(1.734363, 2.968827)$. We can be 95% confident that the odds of testing positive for diabetes for a woman with a BMI at the 3rd quartile are between 1.73 and 2.96 times higher than the odds of testing positive for diabetes for a woman with a BMI at the 1st quartile, holding all other predictors constant.


### Q3.8 

Do women who test positive have higher diastolic blood pressures? Is the diastolic blood pressure significant in the regression model? Explain the distinction between the two questions and discuss why the answers are only apparently contradictory.
```{r}
pima1 <- pima1 %>%
  drop_na(diastolic)
dtm_d <- glm(test_f ~ pregnant + glucose + bmi + diabetes + age + diastolic, 
             family = binomial, data = pima1)
summary(dtm_d)
ggplot(pima1, aes(x = diastolic, fill = test_f2)) +
  geom_histogram(alpha =.5, position = "identity") +
  ylab("Count") + xlab("Diastolic blood pressure (mm Hg)") +
  ggtitle("Histogram of Diastolic BP by Diabetes Test Results") +
  theme_bw()
```
Since the coefficient for `diastolic` in the regression model is negative, that means women with diabetes tend to have lower diastolic blood pressures. However the coefficient is nearly zero, so it seems that their diastolic blood pressures are not that different. Diastolic blood pressure is not significant in the regression model and was not selected in the previous variable selection. This is unusual and contrary to studies that clearly link diastolic blood pressure to diabetes tests. This is likely because diastolic blood pressure is highly correlated with some of the other variables so it gets removed from the model. 
