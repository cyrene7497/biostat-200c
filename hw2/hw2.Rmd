---
title: "Biostat 200C Homework 2"
subtitle: Due Apr 28 @ 11:59PM
output: 
  html_document:
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

To submit homework, please upload both Rmd and html files to Bruinlearn by the deadline.
```{r}
sessionInfo()
```
```{r}
library(faraway)
library(dplyr)
```

## Q1. Case Fatality Rate of COVID-19

Of primary interest to public is the risk of dying from COVID-19. A commonly used measure is case fatality rate/ratio/risk (CFR), which is defined as
$$
\frac{\text{number of deaths from disease}}{\text{number of diagnosed cases of disease}}.
$$
Apparently CFR is not a fixed constant; it changes with time, location, and other factors. Also CFR is different from the infection fatality rate (IFR), the probability that someone infected with COVID-19 dies from it. 

In this exercise, we use logistic regression to study how US county-level CFR changes according to demographic information and some health-, education-, and economy-indicators.

### Data sources

- `04-04-2020.csv.gz`: The data on COVID-19 confirmed cases and deaths on 2020-04-04 is retrieved from the [Johns Hopkins COVID-19 data repository](https://github.com/CSSEGISandData/COVID-19). It was downloaded from this [link](https://github.com/CSSEGISandData/COVID-19) (commit 0174f38). This repository has been archived by the owner on Mar 10, 2023. It is now read-only. You can download data from box: <https://ucla.box.com/s/brb3vz4nwoq8pjkcutxncymqw583d39l>

- `us-county-health-rankings-2020.csv.gz`: The 2020 County Health Ranking Data was released by [County Health Rankings](https://www.countyhealthrankings.org). The data was downloaded from the [Kaggle Uncover COVID-19 Challenge](https://www.kaggle.com/roche-data-science-coalition/uncover) (version 1). You can download data from box: <https://ucla.box.com/s/brb3vz4nwoq8pjkcutxncymqw583d39l>

### Sample code for data preparation

Load the `tidyverse` package for data manipulation and visualization.
```{r}
# tidyverse of data manipulation and visualization
library(tidyverse)
```
Read in the data of COVID-19 cases reported on 2020-04-04.
```{r}
county_count <- read_csv("./datasets/04-04-2020.csv.gz") %>%
  # cast fips into dbl for use as a key for joining tables
  mutate(FIPS = as.numeric(FIPS)) %>%
  filter(Country_Region == "US") %>%
  print(width = Inf)
```
Standardize the variable names by changing them to lower case.
```{r}
names(county_count) <- str_to_lower(names(county_count))
```
Sanity check by displaying the unique US states and territories:
```{r}
county_count %>%
  dplyr::select(province_state) %>%
  distinct() %>%
  arrange(province_state) %>%
  print(n = Inf)
```
We want to exclude entries from `Diamond Princess`, `Grand Princess`, `Guam`, `Northern Mariana Islands`, `Puerto Rico`, `Recovered`, and `Virgin Islands`, and only consider counties from 50 states and DC.
```{r}
county_count <- county_count %>%
  filter(!(province_state %in% c("Diamond Princess", "Grand Princess", 
                                 "Recovered", "Guam", "Northern Mariana Islands", 
                                 "Puerto Rico", "Virgin Islands"))) %>%
  print(width = Inf)
```
Graphical summarize the COVID-19 confirmed cases and deaths on 2020-04-04 by state.
```{r}
county_count %>%
  # turn into long format for easy plotting
  pivot_longer(confirmed:recovered, 
               names_to = "case", 
               values_to = "count") %>%
  group_by(province_state) %>%
  ggplot() + 
  geom_col(mapping = aes(x = province_state, y = `count`, fill = `case`)) + 
  # scale_y_log10() + 
  labs(title = "US COVID-19 Situation on 2020-04-04", x = "State") + 
  theme(axis.text.x = element_text(angle = 90))
```

Read in the 2020 county-level health ranking data.
```{r}
county_info <- read_csv("./datasets/us-county-health-rankings-2020.csv.gz") %>%
  filter(!is.na(county)) %>%
  # cast fips into dbl for use as a key for joining tables
  mutate(fips = as.numeric(fips)) %>%
  dplyr::select(fips, 
         state,
         county,
         percent_fair_or_poor_health, 
         percent_smokers, 
         percent_adults_with_obesity, 
         # food_environment_index,
         percent_with_access_to_exercise_opportunities, 
         percent_excessive_drinking,
         # teen_birth_rate, 
         percent_uninsured,
         # primary_care_physicians_rate,
         # preventable_hospitalization_rate,
         # high_school_graduation_rate,
         percent_some_college,
         percent_unemployed,
         percent_children_in_poverty,
         # `80th_percentile_income`,
         # `20th_percentile_income`,
         percent_single_parent_households,
         # violent_crime_rate,
         percent_severe_housing_problems,
         overcrowding,
         # life_expectancy,
         # age_adjusted_death_rate,
         percent_adults_with_diabetes,
         # hiv_prevalence_rate,
         percent_food_insecure,
         # percent_limited_access_to_healthy_foods,
         percent_insufficient_sleep,
         percent_uninsured_2,
         median_household_income,
         average_traffic_volume_per_meter_of_major_roadways,
         percent_homeowners,
         # percent_severe_housing_cost_burden,
         population_2,
         percent_less_than_18_years_of_age,
         percent_65_and_over,
         percent_black,
         percent_asian,
         percent_hispanic,
         percent_female,
         percent_rural) %>%
  print(width = Inf)
```

For stability in estimating CFR, we restrict to counties with $\ge 5$ confirmed cases.
```{r}
county_count <- county_count %>%
  filter(confirmed >= 5)
```
We join the COVID-19 count data and county-level information using FIPS (Federal Information Processing System) as key. 
```{r}
county_data <- county_count %>%
  left_join(county_info, by = "fips") %>%
  print(width = Inf)
```
Numerical summaries of each variable:
```{r}
summary(county_data)
```
List rows in `county_data` that don't have a match in `county_count`:
```{r}
county_data %>%
  filter(is.na(state) & is.na(county)) %>%
  print(n = Inf)
```
We found there are some rows that miss `fips`. 
```{r}
county_count %>%
  filter(is.na(fips)) %>%
  dplyr::select(fips, admin2, province_state) %>%
  print(n = Inf)
```
We need to (1) manually set the `fips` for some counties, (2) discard those `Unassigned`, `unassigned` or `Out of`, and (3) try to join with `county_info` again.
```{r}
county_data <- county_count %>%
  # manually set FIPS for some counties
  mutate(fips = ifelse(admin2 == "DeKalb" & province_state == "Tennessee", 47041, fips)) %>%
  mutate(fips = ifelse(admin2 == "DeSoto" & province_state == "Florida", 12027, fips)) %>%
  #mutate(fips = ifelse(admin2 == "Dona Ana" & province_state == "New Mexico", 35013, fips)) %>% 
  mutate(fips = ifelse(admin2 == "Dukes and Nantucket" & province_state == "Massachusetts", 25019, fips)) %>% 
  mutate(fips = ifelse(admin2 == "Fillmore" & province_state == "Minnesota", 27045, fips)) %>%  
  #mutate(fips = ifelse(admin2 == "Harris" & province_state == "Texas", 48201, fips)) %>%  
  #mutate(fips = ifelse(admin2 == "Kenai Peninsula" & province_state == "Alaska", 2122, fips)) %>%  
  mutate(fips = ifelse(admin2 == "LaSalle" & province_state == "Illinois", 17099, fips)) %>%
  #mutate(fips = ifelse(admin2 == "LaSalle" & province_state == "Louisiana", 22059, fips)) %>%
  #mutate(fips = ifelse(admin2 == "Lac qui Parle" & province_state == "Minnesota", 27073, fips)) %>%  
  mutate(fips = ifelse(admin2 == "Manassas" & province_state == "Virginia", 51683, fips)) %>%
  #mutate(fips = ifelse(admin2 == "Matanuska-Susitna" & province_state == "Alaska", 2170, fips)) %>%
  mutate(fips = ifelse(admin2 == "McDuffie" & province_state == "Georgia", 13189, fips)) %>%
  #mutate(fips = ifelse(admin2 == "McIntosh" & province_state == "Georgia", 13191, fips)) %>%
  #mutate(fips = ifelse(admin2 == "McKean" & province_state == "Pennsylvania", 42083, fips)) %>%
  mutate(fips = ifelse(admin2 == "Weber" & province_state == "Utah", 49057, fips)) %>%
  filter(!(is.na(fips) | str_detect(admin2, "Out of") | str_detect(admin2, "Unassigned"))) %>%
  left_join(county_info, by = "fips") %>%
  print(width = Inf)
```
Summarize again
```{r}
summary(county_data)
```
If there are variables with missing value for many counties, we go back and remove those variables from consideration.

Let's create a final data frame for analysis.
```{r}
county_data <- county_data %>%
  mutate(state = as.factor(state)) %>%
  dplyr::select(county, confirmed, deaths, state, percent_fair_or_poor_health:percent_rural)
summary(county_data)
```
Display the 10 counties with highest CFR.
```{r}
county_data %>%
  mutate(cfr = deaths / confirmed) %>%
  dplyr::select(county, state, confirmed, deaths, cfr) %>%
  arrange(desc(cfr)) %>%
  top_n(10)
```
Write final data into a csv file for future use.
```{r}
write_csv(county_data, "./datasets/covid19-county-data-20200404.csv.gz")
```

### Note:

Given that the datasets were collected in the middle of the pandemic, what assumptions of CFR might be violated by defining CFR as `deaths/confirmed` from this data set? 

Because COVID-19 pandemic was still ongoing in 2020, we should realize some critical assumptions for defining CFR are not met using this dataset.

1. Numbers of confirmed cases do not reflect the number of diagnosed people. This is mainly limited by the availability of testing.

2. Some confirmed cases may die later.

With acknowledgement of these severe limitations, we continue to use `deaths/confirmed` as a very rough proxy of CFR.

### Q1.1 

Read and run above code to generate a data frame `county_data` that includes county-level COVID-19 confirmed cases and deaths, demographic, and health related information. 

### Q1.2 

What assumptions of logistic regression may be violated by this data set?

One of the assumptions of logistic regression is that the observations are independent from one another. In this case since the observations are counties and cases of COVID, they may not be independent in counties that are adjacent to one another since a high count of cases in one county might affect counts in the adjacent counties.

### Q1.3 

Run a logistic regression, using variables `state`, ..., `percent_rural` as predictors. 

```{r}
lmod <- glm(cbind(deaths, confirmed - deaths) ~ . - county, family = binomial, data = county_data)
summary(lmod)
```


### Q1.4

Interpret the regression coefficients of 3 significant predictors with p-value <0.01.

- A one percent increase in population less than 18 years of age decreases the odds of death among confirmed cases of COVID by roughly 5% on average (exp(-4.812e-02) = 0.9530), holding all else constant in the state of Alabama. 

- A one percent increase in fair or poor health in Alabama increases the odds of death among confirmed cases of COVID by .3% on average (exp(0.003092) = 1.003), holding all else constant.

- COVID cases occurring in the District of Columbia have a .9% increase in odds of death as compared to other states holding all else constant. 

### Q1.5 

Apply analysis of deviance to (1) evaluate the goodness of fit of the model and (2) compare the model to the intercept-only model. 

```{r}
pchisq(lmod$deviance, lmod$df.residual, lower.tail = FALSE)
```
The small p-value suggests the model is not well-calibrated. This does **not** mean that the predictors are insignificant of the odds of death out of COVID cases in a county -- rather, it means our model has been "miss-calibrated" or "miss-specified" in some way. For example, it could mean that we are missing many other important predictors in our model, or it could mean that a model assumption has not been reasonably satisfied.
```{r}
pchisq(lmod$null.deviance - lmod$deviance, lmod$df.null - lmod$df.residual, 
       lower.tail = FALSE)
```
The small p-value suggests that the model performs better than the intercept-only model. 

### Q1.6

Perform analysis of deviance to evaluate the significance of each predictor. Display the 10 most significant predictors.

```{r}
aod <- drop1(lmod, test = "Chi")
arrange(aod, `Pr(>Chi)`) %>%
  head(10)
```
The top ten most significant predictors are `state`, `percent_homeowners`, `percent_insufficient_sleep`, `percent_severe_housing_problems`, `percent_children_in_poverty`, `median_household_income`, `percent_some_college`, `percent_fair_or_poor_health`, `percent_less_than_18_years_of_age`, and `percent_smokers`. 

### Q1.7

Construct confidence intervals of regression coefficients.
```{r}
library(gtsummary)
lmod %>%
  tbl_regression(intercept = TRUE, exponentiate = TRUE) %>%
  suppressWarnings()
```


### Q1.8 

Plot the deviance residuals against the fitted values. Are there potential outliers?
```{r}
county_data %>%
    mutate(devres = residuals(lmod, type = "deviance"),
    linpred = predict(lmod, type = "link")) %>%
    ggplot +
        geom_point(mapping = aes(x = linpred, y = devres)) +
        labs(x = "Linear predictor", y = "Deviance residual")
```
It looks like there is on potential outlier with a residual less than -5.0 and a group of observations that all fall around deviance residual of 0. The counties with residuals near zero are also potentially outliers. Because of these observations where there is no death at all, the predicted probability would be very small and close to zero.

### Q1.9

Plot the half-normal plot. Are there potential outliers in predictor space?

```{r}
halfnorm(hatvalues(lmod))
```
```{r}
county_data %>%
  slice(367, 931)
```
There are two observations in the predictor space that are potential outliers. The counties of District of Columbia and New York both have a percent with access to exercise opportunities of 100% which is unusual. It also might be unusual that these counties are 0% rural which might make them outliers in the predictor space.

### Q1.10

Find the best sub-model using the AIC criterion.
```{r}
stats::step(lmod, trace = TRUE)
```
AIC Criterion dropped 7 variables -- `percent_adults_with_obesity`, `percent_with_access_to_exercise_opportunities`, `overcrowding`, `average_traffic_volume_per_meter_of_major_roadways`, `percent_65_and_over`, `percent_asian` and `percent_rural`. It's strange that overcrowding was dropped since that seems very related to COVID spread, but it must not be indicative of **dying** from a diagnosis of COVID. 

### Q1.11

Find the best sub-model using the lasso with cross validation. 

`glmnet` doesn't have the functionality for binomial but has for bernoulli outcome variables. We first transform the dataset into a format with an `obs_wt` to make it a long format similar to a binomial outcome variable. 

```{r}
obs_wt = c(rbind(county_data$deaths, county_data$confirmed - county_data$deaths))
county_data_long <- county_data %>%
  slice(rep(1:n(), each = 2)) %>% # replicate each row twice
  mutate(deaths = rep(c(1, 0), 1446)) %>%
  mutate(obs_wt = obs_wt)
county_data_long$deaths <- as_factor(county_data_long$deaths)
county_data_long %>% print(width = Inf)
```
```{r}
library(tidymodels)
```
```{r}
# For reproducibility
set.seed(200)

data_split <- initial_split(
  county_data_long, 
  # stratify by deaths
  strata = "deaths", 
  prop = 0.70
  )
data_split
```
```{r}
county_other <- training(data_split)
dim(county_other)
county_test <- testing(data_split)
dim(county_test)
```
```{r}
logit_recipe <- 
  recipe(
    deaths ~ . , 
    data = county_other
  ) %>%
  step_rm(county) %>%
  step_rm(confirmed) %>%
  step_rm(obs_wt) %>%
  # create traditional dummy variables
  step_dummy(all_nominal_predictors()) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # center and scale numeric data
  step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep(training = county_other, retain = TRUE)
logit_recipe
```
```{r}
logit_mod <- 
  logistic_reg(
    penalty = tune(), 
    mixture = 1 # tune()
  ) %>% 
  set_engine("glmnet", standardize = FALSE)
logit_mod
```
```{r}
logit_wf <- workflow() %>%
  add_recipe(logit_recipe) %>%
  add_model(logit_mod)
logit_wf
```
```{r}
param_grid <- grid_regular(
  penalty(range = c(-6, 3)), 
  # mixture(),
  levels = 100 # c(100, 5)
  )
param_grid
```
```{r}
# Set cross-validation partitions
set.seed(201)

folds <- vfold_cv(county_other, v = 5)
folds
```
```{r}
system.time({
logit_fit <- logit_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
})
```

```{r}
logit_fit
```
```{r}
logit_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = penalty, y = mean)) +
  geom_point() +
  labs(x = "Penalty", y = "CV AUC") +
  scale_x_log10()
```
```{r}
best_logit <- logit_fit %>%
  select_best("roc_auc")
best_logit
```
```{r}
final_wf <- logit_wf %>%
  finalize_workflow(best_logit)
final_wf
```
```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit
```
```{r}
final_fit %>% 
  collect_metrics()
tidy(extract_fit_engine(final_fit)) %>%
  filter(lambda > 0.001 & lambda < 0.0011)
```
This model is quite different from the one selected by AIC. 

## Q2. Odds ratios

Consider a $2 \times 2$ contingency table from a prospective study in which people who were or were not exposed to some pollutant are followed up and, after several years, categorized according to the presence or absence of a disease. Following table shows the probabilities for each cell. The odds of disease for either exposure group is $O_i = \pi_i / (1 - \pi_i)$, for $i = 1,2$, and so the odds ratio is
$$
\phi = \frac{O_1}{O_2} = \frac{\pi_1(1 - \pi_2)}{\pi_2 (1 - \pi_1)}
$$
is a measure of the relative likelihood of disease for the exposed and not exposed groups.

|             | Diseased | Not diseased |
|:-----------:|----------|--------------|
| Exposed     | $\pi_1$  | $1 - \pi_1$  |
| Not exposed | $\pi_2$  | $1 - \pi_2$  |

### Q2.1

For the simple logistic model
$$
\pi_i = \frac{e^{\beta_i}}{1 + e^{\beta_i}}, 
$$
show that if there is no difference between the exposed and not exposed groups (i.e., $\beta_1 = \beta_2$), then $\phi = 1$.

**Solution:** 

If there is no difference between the exposed and not exposed groups, then $\beta_1 = \beta_2$. Then, 

\begin{eqnarray*}
\pi_1 $=$ \frac{e^{\beta_1}}{1 + e^{\beta_1}} $=$ \frac{e^{\beta_2}}{1 + e^{\beta_2}} $=$ \pi_2 \\
\pi_1 $=$ \pi_2
\end{eqnarray*}

Then, 

\begin{eqnarray*}
\phi $=$ \frac{O_1}{O_2} $=$ \frac{\pi_1(1 - \pi_2)}{\pi_2 (1 - \pi_1)} \\
$=$ \frac{\pi_1(1 - \pi_1)}{\pi_1 (1 - \pi_1)} $=$ 1
\end{eqnarray*}

### Q2.2 

Consider $J$ $2 \times 2$ tables, one for each level $x_j$ of a factor, such as age group, with $j=1,\ldots, J$. For the logistic model
$$
\pi_{ij} = \frac{e^{\alpha_i + \beta_i x_j}}{1 + e^{\alpha_i + \beta_i x_j}}, \quad i = 1,2, \quad j= 1,\ldots, J.
$$
Show that $\log \phi$ is constant over all tables if $\beta_1 = \beta_2$.

**Solution** 

\begin{eqnarray*}
\pi_{1j} = \frac{e^{\alpha_1 + \beta_1 x_j}}{1 + e^{\alpha_1 + \beta_1 x_j}} & 
\pi_{2j} = \frac{e^{\alpha_2 + \beta_2 x_j}}{1 + e^{\alpha_2 + \beta_2 x_j}} \\
\phi = \frac{\pi_1(1 - \pi_2)}{\pi_2 (1 - \pi_1)} \\
&=& \frac{e^{\alpha_1 + \beta_1 x_j}}{1 + e^{\alpha_1 + \beta_1 x_j}} \cdot (1 - \frac{e^{\alpha_2 + \beta_2 x_j}}{1 + e^{\alpha_2 + \beta_2 x_j}}) \cdot \frac{1 + e^{\alpha_2 + \beta_2 x_j}}{e^{\alpha_2 + \beta_2 x_j}} \cdot  (1 - \frac{e^{\alpha_1 + \beta_1 x_j}}{1 + e^{\alpha_1 + \beta_1 x_j}})^{-1} \\
&=& \frac{e^{\alpha_1 + \beta_1 x_j}}{1 + e^{\alpha_1 + \beta_1 x_j}} \cdot \frac{1}{1 + e^{\alpha_2 + \beta_2 x_j}} \cdot \frac{1 + e^{\alpha_2 + \beta_2 x_j}}{e^{\alpha_2 + \beta_2 x_j}} \cdot (1 + e^{\alpha_1 + \beta_1 x_j}) \\
&=& \frac{e^{\alpha_1 + \beta_1 x_j}}{e^{\alpha_2 + \beta_2 x_j}} = \frac{e^{\alpha_1} \cdot e^{\beta_1 xj}}{e^{\alpha_2} \cdot e^{\beta_2 x_j}} \\
&=& \frac{e^{\alpha_1}}{e^{\alpha_2}} \text{because \beta_1 = \beta_2} \\
\log \phi = \log{e^{\alpha_1}} - \log{e^{\alpha_2}} = \alpha_1 - \alpha_2
\end{eqnarray*}

All $x_j$s cancelled out so the expression is no longer dependent on $j$. Therefore this is true for all $j$ and $\log \phi$ is constant over all tables if $\beta_1 = \beta_2$.

## Q3. ELMR Chapter 4 Excercise 3

The `infert` dataset presents data from a study of secondary infertility (failure to conceive after at least one previous conception). The factors of interest are induced abortions and spontaneous abortions (e.g., miscarriages). The study matched each case of infertility with two controls who were not infertile, matching on age, education and parity (number of prior pregnancies).
```{r}
infert %>%
  print(width = Inf)
```
### Q3.1
Construct cross-classified tables by number of spontaneous and induced abortions separately for cases and controls. Comment on the differences between the two tables.
```{r}
infert %>%
xtabs( ~ spontaneous + induced + case, .) %>%
  print() %>% 
  prop.table()
```
There are many more control cases with no spontaneous or induced abortions than those in the case group. Proportionally both case and control groups have no observations of women with 2 or more spontaneous abortions as well as 2 or more induced abortions.  

### Q3.2 
Fit a binary response model with only spontaneous and induced as predictors. Determine the statistical significance of these predictors. Express the effects of the predictors in terms of odds.
```{r}
m1 <- glm(case ~ spontaneous + induced, family = binomial, infert)
summary(m1)
```
A one unit increase in the number of spontaneous abortions increases the odds of failing to conceive after at least one previous conception (classified as an infertility case) by `r exp(coef(m1)[2])` times (3.3 fold increase in odds). A one unit increase in the number of induced abortions increases the odds of being classified as an infertility case by `r exp(coef(m1)[3])` times (1.5 fold increase in odds or 50% increase). 

### Q3.3
Fit a binary response model with only education, age and parity as predictors. Explain how the significance (or lack thereof) of these predictors should be interpreted.

### Q3.4 
Now put all five predictors in a binary response model. Interpret the results in terms of odds.

### Q3.5 
Fit a matched case control model appropriate to the data. Interpret the output and compare the odds to those found in the previous model.

### Q3.6
The spontaneous and induced predictors could be viewed as ordinal due to the grouping in the highest level. Refit the model using ordinal factors rather than numerical variables for these two predictors. Is there evidence that the ordinal representation is necessary?

